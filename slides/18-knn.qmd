---
title: 'Data Splitting and K-Nearest Neighbors `r fontawesome::fa("people-group")`'
subtitle: "MATH/COSC 3570 Introduction to Data Science"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    #     - "macros.tex"
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    highlight-style: arrow
    code-block-bg: true
    self-contained: false
    slide-number: c/t    
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[math3570-s25.github.io/website](https://math3570-s25.github.io/website/)"
    theme: ["simple", "slides.scss"]
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    code-line-numbers: true
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: true
---

#  {visibility="hidden"}

\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}



```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(emo)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(fontawesome)
library(rmarkdown)
library(reticulate)
library(lattice)
library(openintro)
library(plotly)
library(ggridges)
library(patchwork)
library(skimr)
library(class)
library(caret)
set.seed(1234)
# library(ISLR2)
# library(genridge)
# library(glmnet)
# library(gam)
# library(splines)
# library(MASS)

# library(ElemStatLearn)
knitr::opts_chunk$set(
    fig.asp = 0.618,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "images/18-knn/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 12,
  tibble.width = 80,
  width = 80,
  digits = 3
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

theme_set(theme_minimal(base_size = 20))
```




# {background-color="#A7D5E8" background-image="https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png" background-size="30%" background-position="90% 50%"}

::: {.left}
<h1> Training and Test Data </h1>
:::



## Prediction

- Goal: Build a good regression function or classifier in terms of **prediction accuracy**.


. . .

- The mechanics of prediction is **easy**:
  - Plug in values of predictors to the model equation.
  - Calculate the predicted value of the response $\hat{y}$

<!-- ??? -->
<!-- - So, the mechanics of prediction is **easy**: -->
<!--   - Once you have your model, you can Plug in values of predictors to the model equation -->
<!--   - Calculate the predicted value of the response variable, $\hat{y}$, either numerical or categorical. -->

 
. . .

- Getting it right is **hard**! **No guarantee that**
  - the model estimates are close to the truth
  - your model performs as well with new data (**test** data) as it did with your sample data (**training** data)
  
  
<!-- ??? -->
<!-- - But Getting it right is **hard**! -->
<!--   - There is no guarantee the model estimates you have are correct -->
<!--   - Or that your model will perform as well with new data as it did with your sample data -->
<!-- - Test data are the new data that are not used for training or fitting our model, but the data we are interested in predicting its value. -->
<!-- - So we care about the prediction performance on the test data much more than the performance on the training data. -->



## Spending Our Data

- Several steps to create a useful model: 
    + Parameter estimation
    + Model selection
    + Performance assessment, etc.

. . .

- Doing all of this using the entire data may lead to **overfitting**: 

<br>

> [*The model performs well on the training data, but awfully predicts the response on the new data we are interested.*]{.green}

<br>


<!-- - **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount to the model parameter estimation only (what we've done so far). -->


<!-- ??? -->
<!-- - When we are doing modeling, we are doing several steps to create a useful model,  -->
<!--     + parameter estimation -->
<!--     + model selection -->
<!--     + performance assessment, etc. -->
<!-- - Doing all of this on the entire data we have available may lead to **overfitting**. In classification, it means that our model labels the training response variable almost perfectly with very high classification accuracy, but the model performs very bad when fitted to the test data or incoming future emails for example. -->

<!-- - What we wanna do is to  **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount of the data to the model parameter estimation only which is exactly what we've done so far. Remember in linear regression, we also use the entire data set to train our linear regression model, and estimate the regression coefficients. -->
<!-- - But now, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. And how? -->



## Overfitting {visibility="hidden"}

> [*The model performs well on the training data, but awfully predicts the response on the new data we are interested.*]{.green}

- **Low error rate on observed data, but high prediction error rate on future unobserved data!**

:::{.xsmall}
```{r}
#| purl: false
#| echo: false
#| fig-cap: "Source: modified from https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png"
#| out-width: 60%
knitr::include_graphics("./images/18-knn/overfit.png")
```
:::

::: notes

https://i.pinimg.com/originals/72/e2/22/72e222c1542539754df1d914cb671bd7.png
- Look at this illustration, and let's focus on the overfitting and classification case.
- the blue and red points are our training data representing two categories, and the green points are the new data to be classified.
- the black curve is the classification boundary that separates the two categories.
- Based on the boundary, you can see that the classification performance on the training data set is perfect, because all blue points and red points are perfectly separated.
- However, such classification rule generated by the training data may not be good for the new data.
- With this boundary, ...
- OK so, if we wanna make sure that our model is good at predicting things, we probably want to avoid overfitting. 
- But how?

:::



## Splitting Data

- Often, we don't have another unused data to assess the performance of our model.

- Solution: Pretend we have new data by splitting our data into **training set** and **test set (validation set)**!

. . .

- **Training set:**
    - Sandbox for model building 
    - Spend most of our time using the training set to develop the model
    - Majority of the original sample data (~80%)
    

. . .

- **Test set:**
    - Held in reserve to determine efficacy of one or two chosen models
    - Critical to look at it *once only*, otherwise it becomes part of the modeling process
    - Remainder of the data (~20%)
  
::: notes

- **Allocate specific subsets of data for different tasks**, as opposed to allocating the largest possible amount to the model parameter estimation only (what we've done so far).
- Well we do this by splitting our data. So we split our data into to sets, training set and testing set, or sometimes called validation set.
- You can think about your training set as your sandbox for model building. You can do whatever you want, like data wrangling, data transformation, data tidying, and data visualization, all of which help you build an appropriate model.
- So you Spend most of your time using the training set to develop the model
- And this is the Majority of the original sample data, which is usually about 75% - 80% of your data. So you basically take a random sample from the data that is about 80% of it. 
- And you don't touch the remaining 20% of the data until you are ready to test your model performance.
- So the test set is held in reserve to determine efficacy of one or two chosen models
- Critical to look at it once, otherwise it becomes part of the modeling process
- and that is the Remainder of the data, usually 20% - 25%
- So ideally, we hope to use our entire data as training data to train our model, right? And to test the model performance, we just collect another data set as test data to be used for testing performance. But in reality, it is not the usual case. In reality, we only have one single data set, and it is hard to collect another sample data as test data.
- So under this situation, this type of splitting data becomes a must if we want to have both training and test data. 

:::



## initial_split() in [![rsample](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png){width=60}](https://rsample.tidymodels.org/)


```{r}
#| code-fold: true
#| label: bodydata
#| code-line-numbers: false
bodydata <- read_csv("./data/body.csv")
body <- bodydata |> 
    select(GENDER, HEIGHT, WAIST, BMI) |> 
    mutate(GENDER = as.factor(GENDER))
```

<br>


:::: {.columns}

::: {.column width="50%"}

```{r}
#| code-line-numbers: false
set.seed(2025)
df_split <- 
    rsample::initial_split(
        data = body, 
        prop = 0.8)

df_split
```

:::

::: {.column width="50%"}

```{r}
#| code-line-numbers: false
df_trn <- rsample::training(df_split)
df_tst <- rsample::testing(df_split)

dim(df_trn)
dim(df_tst)
```

:::

::::


::: notes
names(df_split)

df_split$in_id |> head()
:::


## `body` Data

:::: {.columns}

::: {.column width="50%"}
```{r}
df_trn
```
:::



::: {.column width="50%"}
```{r}
df_tst
```
:::
::::



## What Makes a Good Classifier: Test Accuracy Rate

- The **test accuracy rate** associated with the *test data* $\{x_j, y_j\}_{j=1}^J$:

$$ \frac{\text{the number of correct predictions, i.e., } y_j = \hat{y}_j}{J},$$


<!-- $$ \frac{1}{J}\sum_{j=1}^JI(y_j = \hat{y}_j),$$ -->
where $\hat{y}_j$ is the predicted label resulting from applying the classifier to the test response $y_j$ with predictor $x_j$.


:::{.question}
What is the value of $J$ in our example?
:::

. . .

- The *best* estimated **classifier** $\hat{C}(x)$ trained from the training data for $C(x)$ can be defined as the one producing the *highest test accuracy rate* or *lowest test error rate*.



## K-Nearest Neighbors (KNN) Classifier

KNN classification uses *majority voting*:

:::{.center}
[*Look for the most popular class label among its neighbors*.]{.green}
:::

<!-- - $\pi_{K}(x) = \hat{P}(Y = 1 \mid X = x) = \frac{1}{K}\sum_{i \in \mathcal{N}_x}I(y_i = 1)$ -->

:::: {.columns}

::: {.column width="50%"}

<!-- - *Neighbors in __Euclidean distance__ sense.* -->


```{r}
#| fig-asp: 1
#| out-width: 80%
#| echo: false
#| cache: true
par(mar = c(3, 4, 2, 0), mgp = c(2, 0.8, 0))
set.seed(42)
x <- matrix(rnorm(40, mean = 3, sd = 3), 40/2,2)
class <- rep(c(1, 2),each = 40/4)
x[class == 1,] = x[class == 1,] + 3
# plot(x,col = c("orangered", "navyblue")[class],pch = 20, xlab = "x1", ylab = "x2")

knn_ex = tibble::tibble(
    x1 = x[, 1],
    x2 = x[, 2],
    class = class)
plot(x2 ~ x1, col = c("orange", "navyblue")[knn_ex$class], data = knn_ex,
     ylim = range(x2), xlim = range(x1), pch = 20,
     cex = 5, las = 1, cex.main = 3, cex.lab = 2, cex.axis = 2,
     main = "KNN: K = 3")
points(8, 6, col = "darkgrey", pch = "*", cex = 8)
plotrix::draw.circle(8, 6, 2, 
                     nv = 1000, lty = 1, 
                     lwd = 2, 
                     border = "darkgrey",
                     col = alpha("lightgreen", 0.2))
abline(v = 8, lty = 2, lwd = 0.5)
abline(h = 6, lty = 2, lwd = 0.5)
# legend("bottomleft", c("O", "B"), pch = c(20, 20),
#        col = c("darkorange", "dodgerblue"))
```

:::



::: {.column width="50%"}

When predicting at $x = (x_1, x_2) = (8, 6)$,


\begin{align}
\hat{\pi}_{3Blue}(x = (8, 6)) &= \hat{P}(Y = \text{Blue} \mid x = (8, 6))\\
&= \frac{2}{3}
\end{align}

\begin{align}
\hat{\pi}_{3Orange}(x = (8, 6)) &= \hat{P}(Y = \text{Orange} \mid x = (8, 6))\\
&= \frac{1}{3}
\end{align}
<!-- Thus -->
<!-- $\hat{C}_3(x = (8, 6)) = \text{Blue}$ -->


:::

::::

::: notes

- Here is a graphical example. Suppose K = 3 and we have two predictors, $x_1$ and $x_2$. We want to do classification of $Y$ when $x_1$ is 8 and $x_2$ is 6.
- Here how do we define neighbors, we use Euclidean distance to decide who are the point (8, 6)'s neighbors. 
- Showing the idea in the figure, we just use the point (8, 6) as the center of a circle, draw a circle with a larger and larger radius until the circle captures 3 other data points that will be treated as neighbors.
- Here you can see the green circle captures the points, two are blue and one is orange.
- So when we do classification at (8, 6), we just compute the proportion of blue neighbors and the proportion of the orange neighbors, and assign the category with the highest proportion or probability to the response variable at the value of predictors (8, 6).

:::




## K-Nearest Neighbors {visibility="hidden"}

To predict the category of $y$ at $X = x$, with $y = 0, 1$, `0` being Orange; `1` being Blue

- [Logistic regression]{.green}: $$\hat{\pi}(x) = \hat{P}(Y = 1 \mid X = x) = \frac{1}{1+e^{-\hat{\beta_0}-\hat{\beta_1}x}}$$

- [KNN]{.green}: $$\hat{\pi}_{K}(x) = \hat{P}(Y = 1 \mid X = x) = \frac{1}{K} \sum_{i \in \mathcal{N}_K(x)}I(y_i = 1),$$ where $\mathcal{N}_K(x)$ is the collection of indexes for which the training points $x_i$s are the $K$ neighbors of $x$.


::: notes

- Let's see what K-Nearest Neighbors method or KNN is.
- For the binary logistic regression, 
$p(x) = \hat{P}(Y = 1 \mid X = x) = \frac{e^{\hat{\beta_0}+\hat{\beta_1}x_1 + \dots + \hat{\beta_p}x_p}}{1+e^{\hat{\beta_0}+\hat{\beta_1}x_1 + \dots + \hat{\beta_p}x_p}}$. Model parameters include $\beta_0, \dots, \beta_p$. So linear regression and logistic regression are both parametric approaches. These models have some parameters to be estimated.
- KNN first identifies the $K$ points that is closest to $x$, or the K nearest neighbors of $x$, then estimates the probability $p_{Km}(x)$ as
$p_{Km}(x) = \hat{P}(Y = m \mid X = x) = \frac{1}{K}\sum_{i \in \mathcal{N}_x}I(y_i = m),$ where $\mathcal{N}_x$ is the collection of indexes that $x_i$s are the $K$ neighbors of $x$.
- So the idea is that we first decide how many neighbors of $x$ or K to be used, and then compute the proportion of those neighbors whose y label is category $m$
- KNN applies the Bayes rule and classifies the test response to the class with the largest probability. That is, $\hat{C}_K(x) = \underset{m}{\mathrm{argmax}} \ \ p_{Km}(x)$
- In the binary case this becomes
$$\hat{C}_K(x) = \begin{cases}
    0      & p_{Km}(x) < 0.5\\
    1      & p_{Km}(x) > 0.5
\end{cases}.$$
and if the probability for class $0$ and $1$ are equal, simply assign at random.
- We typically avoid this equal probability thing happened by using odd number of K.

:::





## KNN Decision Boundary

- Blue grid indicates the region in which a test response is assigned to the blue class.

- *We don't know the true boundary (the true classification rule)!*.

```{r}
#| fig-asp: 1
#| out-width: 80%
#| echo: false
#| cache: true
par(mar = c(3, 3, 2, 0), mgp = c(2, 0.8, 0))
# make.grid <- function(x,n=200){
#     grange <- apply(x, 2, range)
#     x1=seq(from=grange[1,1],to=grange[2,1],length=n)
#     x2=seq(from=grange[1,2],to=grange[2,2],length=n)
#     expand.grid(X1=x1,X2=x2)
# }
myknn <- function(train, test, trainclass, k) {
  # Loading library
  require(fields)
  # create an empty placeholder for predicted values
  pred = c()
  
  # calculate distance
  # The output of the "dist" dataframe is such that the rows are the 
  #     training data points, while the columns are the testing observations.
  #     The cells for each row-column pair are the Euclidean distance from
  #     training data to the corresponsing testing data
  dist = rdist(train, test)
  
  # Create a loop for each testing observation
  for (i in 1:nrow(test))
  {
  nb = data.frame(dist = dist[,i], class = trainclass)
  
  # Ranking the rows in the dataframe by the distance from the testing
  #   observation. nb stands Neighbourhood
  nb = nb[order(nb$dist),]
  
  # Choose the K closest Neighbour
  topnb = nb[1:k,]
  
  #Deciding the Class by picking the highest occurence name.
  ans = names(sort(summary(topnb$class), decreasing=T)[1])
  
  # concatenate the latest prediction to the previous one
  pred = c(pred, ans)
  }
  return(pred)
}
x1 <- seq(from = min(knn_ex$x1)-1, to = max(knn_ex$x1)+1, length = 100)
x2 <- seq(from = min(knn_ex$x2)-1, to = max(knn_ex$x2)+1, length = 100)
xgrid <- expand.grid(x1, x2)
ygrid <- myknn(knn_ex %>% select(x1, x2), xgrid, as.factor(knn_ex$class), k = 3)
par(mar = c(4, 4, 2, 1))
plot(xgrid, col = c("orange","dodgerblue")[as.numeric(ygrid)], pch = 20, cex = .8, 
     main = "KNN: K = 3 Decision Boundary", xlab = "x1", ylab = "x2", las = 1,
     ylim = range(x2), xlim = range(x1), cex.main = 2, cex.lab = 2)
points(knn_ex %>% select(x1, x2), col = c("orangered", "navyblue")[knn_ex$class], pch = 20, cex = 3.5)
points(8, 6, col = "black", pch = "*", cex = 6)
plotrix::draw.circle(8, 6, 2, 
                     nv = 1000, lty = 1, 
                     lwd = 2, 
                     border = "darkgrey")
```


::: notes

- OK. Now for every possible value of $x_1$ and $x_2$, we can classify its corresponding response, right?
- So we can actually create a dense grid of $x_1$ and $x_2$, and label each point on the grid. 
- And so we can have a whole picture of how the classification result looks like. 
- Blue (Orange) grid indicates the region in which a test observation will be assigned to the blue (orange) class.
- The curves that separates different classes are called decision boundaries.
- Again, in reality, we don't know the true boundary, the Bayesian decision boundary.

:::


## KNN Training [![recipes](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/recipes.png){width=60}](https://recipes.tidymodels.org/)

- [*Step 1: Create recipe*]{.green}: `recipes::recipe()`

**Standardize predictors before doing KNN!**


```{r}
#| include: true
#| code-line-numbers: false
knn_recipe <- recipes::recipe(GENDER ~ HEIGHT, data = df_trn) |> 
    step_normalize(all_numeric_predictors())
```


```{r}
#| eval: false
#| code-line-numbers: false
── Recipe ────────────────────────────────────────────────────────────────

── Inputs 
Number of variables by role
outcome:   1
predictor: 1

── Operations 
• Centering and scaling for: all_numeric_predictors()
```


::: notes

A recipe is a description of the steps to be applied to a data set in order to prepare it for data analysis.
feature engineering steps to get your data ready for modeling

which is designed to help you preprocess your data before training your model

:::



## KNN Training [![parsnip](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){width=60}](https://parsnip.tidymodels.org/reference/nearest_neighbor.html)

- [*Step 2: Specify Model*]{.green}: `parsnip::nearest_neighbor()`

<!-- ```{r} -->
<!-- (knn_mdl <- parsnip::nearest_neighbor(neighbors = 3) |>  -->
<!--     set_mode("classification") |>  -->
<!--     set_engine("kknn")) -->
<!-- ``` -->

```{r}
#| code-line-numbers: false
(knn_mdl <- parsnip::nearest_neighbor(mode = "classification", neighbors = 3))
```


## KNN Training [![workflows](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/workflows.png){width=60}](https://workflows.tidymodels.org/)

- [*Step 3: Fitting by creating workflow*]{.green}: `workflows::workflow()`


<br>

:::: {.columns}

::: {.column width="35%"}

```{r}
#| class-output: my_classfull
#| code-line-numbers: false
knn_out <- 
    workflows::workflow() |> 
    add_recipe(knn_recipe) |> 
    add_model(knn_mdl) |> 
    fit(data = df_trn)
```

:::


::: {.column width="65%"}

::: small

```{r}
#| class-output: my_class600
#| echo: false
#| code-line-numbers: true
knn_out
```

:::

:::

::::


::: notes
A workflow is a container object that aggregates information required to fit and predict from a model.
:::


## Prediction on Test Data

:::: {.columns}

::: {.column width="52%"}

```{r}
#| class-output: my_class600
#| code-line-numbers: false
bind_cols(
  predict(knn_out, df_tst),
  predict(knn_out, df_tst, type = "prob")) |> 
  dplyr::sample_n(size = 8)
```

:::



::: {.column width="48%"}

```{r}
#| code-line-numbers: false
knn_pred <- pull(predict(knn_out, df_tst))

## Confusion matrix
table(knn_pred, df_tst$GENDER)

## Test accuracy rate
mean(knn_pred == df_tst$GENDER)
```

:::

::::

:::  notes
mean(knn_pred != df_tst$GENDER)
:::



## Which K Should We Use?

- $K$-nearest neighbors has no model parameters, but a **tuning parameter** $K$.

- This is a parameter which *determines how the model is trained*, not a parameter that is learned through training.

:::: {.columns}

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-asp: 1
#| out-width: 80%
ygrid <- myknn(knn_ex %>% select(x1, x2), xgrid, as.factor(knn_ex$class), k = 1)
par(mar = c(4, 5, 2, 0))
plot(xgrid, col = c("orange","dodgerblue")[as.numeric(ygrid)], pch = 20, cex = .8, 
     main = "KNN: K = 1", xlab = "x1", ylab = "x2", las = 1,
     ylim = range(x2), xlim = range(x1), cex.main = 2, cex.lab = 2)
points(knn_ex %>% select(x1, x2), col = c("orangered", "navyblue")[knn_ex$class], pch = 20, cex = 3.5)
# points(8, 6, col = "black", pch = "*", cex = 6)
# plotrix::draw.circle(8, 6, 2, 
#                      nv = 1000, lty = 1, 
#                      lwd = 2, 
#                      border = "darkgrey")
```

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-asp: 1
#| out-width: 80%
ygrid <- myknn(knn_ex %>% select(x1, x2), xgrid, as.factor(knn_ex$class), k = 15)
par(mar = c(4, 5, 2, 0))
plot(xgrid, col = c("orange","dodgerblue")[as.numeric(ygrid)], pch = 20, cex = .8, 
     main = "KNN: K = 15", xlab = "x1", ylab = "x2", las = 1,
     ylim = range(x2), xlim = range(x1), cex.main = 2, cex.lab = 2)
points(knn_ex %>% select(x1, x2), col = c("orangered", "navyblue")[knn_ex$class], pch = 20, cex = 3.5)
# points(8, 6, col = "black", pch = "*", cex = 6)
# plotrix::draw.circle(8, 6, 2, 
#                      nv = 1000, lty = 1, 
#                      lwd = 2, 
#                      border = "darkgrey")
```

:::

::::


## $v$-fold Cross Validation

- Use **$v$-fold Cross Validation (CV)** to choose tuning parameters. (MATH 4750 Computational Statistics)

- Usually use $v = 5$ or $10$.

:::: {.columns}

::: {.column width="42%"}

- IDEA: 
  + Prepare $v$ CV data sets
  + Create a sequence of values of $K$
  + For each value of $K$, run CV, and obtain an accuracy rate
  + Choose the $K$ with the highest accuracy rate

:::


::: {.column width="58%"}

```{r}
#| echo: false
#| out-width: 100%
knitr::include_graphics("./images/18-knn/cross-validation.png")
```

:::

::::


  
## [`rsample::vfold_cv()`](https://rsample.tidymodels.org/reference/vfold_cv.html) {visibility="hidden"}

```{r}
## Create folds
set.seed(3570)
(body_vfold <- rsample::vfold_cv(df_trn, v = 5, strata = GENDER))
```

<br>

```{r}
k_val <- tibble(neighbors = seq(from = 1, to = 30, by = 2))
```


## [![tune](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tune.png){width=60}](https://tune.tidymodels.org/) [`tune::tune_grid()`](https://tune.tidymodels.org/reference/tune_grid.html) {visibility="hidden"}

```{r}
#| cache: false
knn_mdl <- parsnip::nearest_neighbor(neighbors = tune()) |> 
    set_mode("classification") |> 
    set_engine("kknn")
```


<br>

```{r}
knn_cv <- workflows::workflow() |> 
    add_recipe(knn_recipe) |> 
    add_model(knn_mdl) |> 
    tune::tune_grid(resamples = body_vfold, grid = k_val) |> 
    tune::collect_metrics()
```

```{r}
#| echo: false
knn_cv |> print(n = 3)
```

<br>

```{r}
accu_cv <- knn_cv |> filter(.metric == "accuracy")
```

::: notes
knn_recipe <- recipes::recipe(GENDER ~ HEIGHT, data = df_trn) |> 
    step_scale(all_predictors()) |> 
    step_center(all_predictors())

knn_mdl <- parsnip::nearest_neighbor(neighbors = tune()) |> 
    set_mode("classification") |> 
    set_engine("kknn")
:::




## Best $K$

```{r}
#| include: false
accu_cv |> slice_max(mean)
```

```{r}
#| echo: false
ggplot(accu_cv, aes(x = neighbors, y = mean)) +
    geom_line() +
  labs(y = "Accuracy", x = "# neighbors: k" )
```



## Final Model

```{r}
#| code-line-numbers: false
knn_mdl_best <- parsnip::nearest_neighbor(neighbors = 29, mode = "classification")

knn_out_best <- workflows::workflow() |>
    add_recipe(knn_recipe) |>
    add_model(knn_mdl_best) |>
    fit(data = df_trn)
```

::: notes
best_K <- accu_cv |> slice_max(mean) |> pull(neighbors) |> as.integer()
:::

## Final Model Performance

```{r}
#| eval: true
#| code-line-numbers: false
knn_pred_best <- pull(predict(knn_out_best, df_tst))

## Confusion matrix
table(knn_pred_best, df_tst$GENDER)

## Test accuracy rate
mean(knn_pred_best == df_tst$GENDER)
```

# {background-color="#ffde57" background-image="https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg" background-size="40%" background-position="90% 50%"}


::: {.left}
<h1> sklearn.neighbors </h1>
<h1> sklearn.model_selection </h1>
:::


## [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)

```{r}
#| eval: false
#| code-line-numbers: false
library(reticulate); py_install("scikit-learn")
```


```{python}
#| eval: true
#| code-line-numbers: false
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
```

. . .

<br>

```{python}
#| eval: true
#| code-line-numbers: false
body = pd.read_csv('./data/body.csv')

X = body[['HEIGHT']]
y = body['GENDER']
X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2025)
```



## [sklearn.neighbors.KNeighborsClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)


```{python}
#| eval: true
#| code-line-numbers: false
knn = KNeighborsClassifier(n_neighbors = 3)
# X_trn = np.array(X_trn)
# X_tst = np.array(X_tst)
knn.fit(X_trn, y_trn)
```

## Prediction

```{python}
#| code-line-numbers: false
y_pred = knn.predict(X_tst)
```


```{python}
#| code-line-numbers: false
from sklearn.metrics import confusion_matrix
confusion_matrix(y_tst, y_pred)
```

. . .

<br>

```{python}
#| code-line-numbers: false
np.mean(y_tst == y_pred)
```


```{python}
#| code-line-numbers: false
np.mean(y_tst != y_pred)
```


##
::: {.lab}

<span style="color:blue"> **22-K Nearest Neighbors** </span>

In **lab.qmd** `## Lab 22` section, 

1. use `HEIGHT` and `WAIST` to predict `GENDER` using KNN with $K = 3$.

2. Generate the (test) confusion matrix.

3. Calculate (test) accuracy rate.

4. Does using more predictors predict better?

```{r}
#| eval: false
```

:::

## R Code

::: midi

```{r}
#| eval: false
#| class-source: my_classfull

library(tidymodels)

## load data
bodydata <- read_csv("./data/body.csv")
body <- bodydata |> 
    select(GENDER, HEIGHT, WAIST) |> 
    mutate(GENDER = as.factor(GENDER))

## training and test data
set.seed(2025)
df_split <- initial_split(data = body, prop = 0.8)
df_trn <- training(df_split)
df_tst <- testing(df_split)

## KNN training
knn_recipe <- recipe(GENDER ~ HEIGHT + WAIST, data = df_trn) |> 
    step_normalize(all_predictors())
knn_mdl <- nearest_neighbor(neighbors = 3, mode = "classification")
knn_out <- workflow() |> 
    add_recipe(knn_recipe) |> 
    add_model(knn_mdl) |> 
    fit(data = df_trn)

## KNN prediction
knn_pred <- pull(predict(knn_out, df_tst))
table(knn_pred, df_tst$GENDER)
mean(knn_pred == df_tst$GENDER)
```

:::



## Python Code

::: midi

```{python}
#| eval: false
#| class-source: my_classfull

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

## load data
body = pd.read_csv('./data/body.csv')
X = body[['HEIGHT', 'WAIST']]
y = body['GENDER']

## training and test data
X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2, random_state=2025)

## KNN training
knn = KNeighborsClassifier(n_neighbors = 3)
# X_trn = np.array(X_trn)
# X_tst = np.array(X_tst)
knn.fit(X_trn, y_trn)

## KNN prediction
y_pred = knn.predict(X_tst)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_tst, y_pred)
np.mean(y_tst == y_pred)
```

:::
