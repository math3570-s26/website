---
title: 'Logistic Regression `r fontawesome::fa("computer")`'
subtitle: "MATH/COSC 3570 Introduction to Data Science"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    #     - "macros.tex"
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    highlight-style: arrow
    code-block-bg: true
    self-contained: false
    slide-number: c/t    
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[math3570-s25.github.io/website](https://math3570-s25.github.io/website/)"
    theme: ["simple", "slides.scss"]
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    code-line-numbers: false
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: true
---

#  {visibility="hidden"}

\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}



```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(emo)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(fontawesome)
library(rmarkdown)
library(reticulate)
library(lattice)
library(openintro)
library(plotly)
# library(ISLR2)
# library(genridge)
# library(glmnet)
# library(gam)
# library(splines)
# library(MASS)

# library(ElemStatLearn)
knitr::opts_chunk$set(
    fig.asp = 0.618,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "images/17-logistic-reg/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 9,
  tibble.width = 80,
  width = 80,
  digits = 3
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

theme_set(theme_minimal(base_size = 20))
```


## Regression vs. Classification


:::: {.columns}

::: {.column width="33%"}

:::{.center}
**Normal vs. Spam/Phishing**
:::

```{r}
#| echo: false
#| out-width: 70%
knitr::include_graphics("./images/17-logistic-reg/spam_filter.jpg")
```
:::

::: {.column width="33%"}

:::{.center}
**Fake vs. True**
:::

```{r}
#| echo: false
#| out-width: 90%
knitr::include_graphics("./images/17-logistic-reg/fake_news.jpeg")
```
:::

::: {.column width="33%"}

:::{.center}
**Normal vs. COVID vs. Smoking**
:::

```{r}
#| echo: false
#| out-width: 90%
knitr::include_graphics("./images/17-logistic-reg/covid_lung.jpeg")
```
:::
::::


- The response $Y$ in linear regression is *numerical*.

- In many situations, $Y$ is *categorical*!

- A process of predicting categorical response is known as **classification**.

::: notes
+ eye color
+ car brand
+ true vs. fake news
:::


## Regression Function $f(x)$ vs. Classifier $C(x)$


```{r}
#| purl: false
#| echo: false
#| out-width: 100%
knitr::include_graphics("./images/17-logistic-reg/regression.png")
```


. . .


:::{.xsmall}
```{r}
#| purl: false
#| echo: false
#| out-width: 100%
#| fig-cap: "Source: https://daviddalpiaz.github.io/r4sl/classification-overview.html"
knitr::include_graphics("./images/17-logistic-reg/classification.png")
```
:::

::: notes
- There are many classification tools, or **classifiers** used to predict a categorical response.
:::


## Classification Example

- Predict whether people will default on their credit card payment $(Y)$ `yes` or `no`, based on monthly credit card balance $(X)$.

- Use the training sample $\{(x_1, y_1), \dots, (x_n, y_n)\}$ to build a classifier.

:::: {.columns}

::: {.column width="60%"}

```{r}
#| purl: false
#| echo: false
#| out-width: 100%
library(ISLR2)
Default_tbl <- as_tibble(Default)
Default_tbl |> 
    ggplot(aes(default, balance, fill = default)) +
    geom_boxplot(color="black") + 
    labs(title = "Default vs. Balance") +
    theme(legend.position="bottom")
```

:::



::: {.column width="40%"}

```{r}
#| purl: false
#| echo: false
#| out-width: 100%
knitr::include_graphics("images/17-logistic-reg/credit_card.jpeg")
```

:::

::::


## Why Not Linear Regression? {visibility="hidden"}

- Most of the time, we code categories using numbers!

$Y =\begin{cases}
    0  & \quad \text{if not default}\\
    1  & \quad \text{if default}
     \end{cases}$
     

- $Y = \beta_0 + \beta_1X + \epsilon$, $\, X =$ credit card balance 

:::{.question}
Any potential issue of this dummy variable approach?
:::

<!-- - Linear regression assumes that the response is -->
<!--   + Normally distributed -->
<!--   + Constant variance -->
<!--   + Independent -->

::: notes
- $Y$ is categorical but coded as dummy variable or indicator variable
- Fit linear regression and treat it as a numerical variable.
:::



## Why Not Linear Regression? {visibility="hidden"}

- $\hat{Y} = b_0 + b_1X$ estimates $P(Y = 1 \mid X) = P(default = yes \mid balance)$
<!-- - Brown bars are data points. -->

```{r lm_default}
#| echo: false
#| out-width: 72%
Default_tbl |>  
    ggplot(aes(x = balance, y = as.numeric(default)-1), colour=default) +
    geom_point(aes(colour=default), alpha = 0.1) + 
    geom_smooth(method = lm, se = FALSE) +
    theme(legend.position = "none") +
    ylab("default") +
    labs(title = "Simple Linear Regression: Default vs. Balance")
```


## Why Not Linear Regression?  {visibility="hidden"}

- $\hat{Y} = b_0 + b_1X$
- **Some estimates might be outside $[0, 1]$.**

```{r, ref.label="lm_default"}
#| echo: false
#| out-width: 72%
```



## Binary Classification by Probability


:::: {.columns}

::: {.column width="50%"}

- Most of the time, we code categories using numbers!

$Y =\begin{cases}
    0  & \quad \text{if not default}\\
    1  & \quad \text{if default}
     \end{cases}$

- First predict the **probability** of each category of $Y$.

- Predict probability of `default` using a <span style="color:blue">**S-shaped** curve</span>.


:::



::: {.column width="50%"}

```{r}
#| echo: false
#| out-width: 100%
#| label: glm_default
Default_tbl |> 
    ggplot(aes(x = balance, y = as.numeric(default) - 1), colour = default) +
    geom_point(aes(colour = default), alpha = 0.1) + 
    geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
    theme(legend.position = "none") +
    ylab("Probability of Default") +
    labs(title = "Simple Logistic Regression: Default vs. Balance")
```

:::

::::



<!-- # Binary Logistic Regression -->


## Framing the Problem: Binary Responses {visibility="hidden"}

- Treat each outcome (`default` $(y = 1)$ and `not default` $(y = 0)$) as success and failure arising from separate **Bernoulli** trials.

:::{.question}
What is a Bernoulli trial?
:::

:::{.question}
What is a binomial trial?
:::

- A Bernoulli trial is a special case of a binomial trial when the number of trials is one:
  - **exactly two** possible outcomes, "success" and "failure"
  - the probability of success $\pi$ is **constant**

:::{.question}
In the default credit card example, 

- do we have exactly two outcomes? 

- do we have constant probability? $P(y_1 = 1) = P(y_2 = 1) = \cdots = P(y_n = 1) = \pi?$

:::




## Framing the Problem: Binary Responses {visibility="hidden"}

- Training data $(x_1, y_1), \dots, (x_n, y_n)$ where 
    + $y_i = 1$ (`default`) 
    + $y_i = 0$ (`not default`).

- We first predict $P(y_i = 1 \mid x_i) = \pi(x_i) = \pi_i$

- The probability $\pi$ *changes with* the value of predictor $x$!

. . .

- $X =$ `balance`. $x_1 = 2000$ has a larger $\pi_1 = \pi(2000)$ than $\pi_2 = \pi(500)$ with $x_2 = 500$.

- Credit cards with a higher balance is more likely to be default.





## Binary Responses with Nonconstant Probability

:::: {.columns}

::: {.column width="40%"}
<!-- - Two outcomes: `default` $(y = 1)$ and `not default` $(y = 0)$ -->

- Training data $(x_1, y_1), \dots, (x_n, y_n)$ where 
    + $y_i = 1$ (`default`) 
    + $y_i = 0$ (`not default`).


- First predict $P(y_i = 1 \mid x_i) = \pi(x_i) = \pi_i$



- [**The probability $\pi$ *changes with* the value of predictor $x$!**]{.blue}

:::



::: {.column width="60%"}


```{r, ref.label="glm_default"}
#| echo: false
#| out-width: 100%
```

:::
::::

. . .

<!-- $y_i \mid x_i \stackrel{indep}{\sim} \text{Bernoulli}(\pi(x_i)) = \text{binomial}(m=1,\pi = \pi(x_i))$ -->

- $X =$ `balance`. $x_1 = 2000$ has a larger $\pi_1 = \pi(2000)$ than $\pi_2 = \pi(500)$ with $x_2 = 500$.

- Credit cards with a higher balance is more likely to be default.



## Logistic Regression {visibility="hidden"}

:::{.center}
Instead of predicting $y_i$ directly, we use predictors to model its *probability* of success, $\pi_i$.
:::

:::{.center}
But how?
:::


. . .

- **Transform $\pi \in (0, 1)$ into another variable $\eta \in (-\infty, \infty)$. Then construct a linear predictor on $\eta$**: $$\eta_i = \beta_0 + \beta_1 x_i$$


. . .

- **Logit function:** For $0 < \pi < 1$

$$\eta = \text{logit}(\pi) = \ln\left(\frac{\pi}{1-\pi}\right)$$

- The *logit* function takes a value $\pi \in (0, 1)$ and maps it to a value $\eta \in (-\infty, \infty)$.



## Logit function $\eta = \text{logit}(\pi) = \ln\left(\frac{\pi}{1-\pi}\right)$ {visibility="hidden"}

```{r}
#| echo: false
#| out-width: 72%
#| cache: true
d <- tibble(p = seq(0.0001, 0.9999, length.out = 2000)) %>%
    mutate(logit_p = log(p/(1-p)))

ggplot(d, aes(x = p, y = logit_p)) + 
    geom_line() + 
    xlim(0,1) + 
    xlab(expression(pi)) + 
    ylab(expression(paste("logit(", pi, ")"))) +
    labs(title = expression(paste("logit(", pi, ") vs. ", pi)))
```




## Logistic Function

- Assume $\pi$ is affected by the linear function $\beta_0 + \beta_1x$ with the **logistic** transformation:

$$\pi(x) = \frac{1}{1+\exp(-(\beta_0 + \beta_1x))}$$

. . .


::: {.question}
Does the logistic function guarantee that $\pi \in (0, 1)$ for any value of $\beta_0$, $\beta_1$, and $x$?
:::

 
 
## Logistic Function {visibility="hidden"}

- The *logistic* function takes a value $\eta \in (-\infty, \infty)$ and maps it to a value $\pi \in (0, 1)$.

- **Logistic function**:
$$\pi = \text{logistic}(\eta) = \frac{1}{1+\exp(-\eta)} \in (0, 1)$$

. . .

- So once $\eta$ is estimated by the linear predictor, we use the logistic function to transform $\eta$ back to the probability.

## Logistic Function $\pi(x) = \text{logistic}(\beta_0 + \beta_1x) = \frac{\exp(\beta_0 + \beta_1 x)}{1+\exp(\beta_0 + \beta_1 x)}$


```{r}
#| echo: false
#| out-width: 72%
#| cache: true
d <- tibble(eta = seq(-5, 5, length.out = 2000)) %>%
    mutate(logistic = (1/(1+exp(-eta))))

ggplot(d, aes(x = eta, y = logistic)) + 
    geom_line() + 
    xlim(-5,5) + 
    xlab(expression(z)) + 
    ylab(expression(paste("logistic(", z, ")"))) +
    labs(title = expression(paste("logistic(", z, ") vs. ", z)))
```


## Simple Binary Logistic Regression Model

For $i = 1, \dots, n$, and with one predictor $X$:
  $$(Y_i \mid X = x_i) = \begin{cases}   1       & \quad \text{with probability } \pi(x_i)\\
    0  & \quad \text{with probability } 1 - \pi(x_i) \end{cases}$$
  <!-- $$\ln \left( \frac{\pi(x_i)}{1 - \pi(x_i)} \right) = \beta_0+\beta_1 x_{i}$$ -->
  
  $$\pi(x_i) = \frac{1}{1+\exp(-(\beta_0+\beta_1 x_{i}))}$$
<!-- - The $\text{logit}(\pi_i)$ is a **link function** that *links* the linear predictor and the mean of $Y_i$. -->

. . .

Goal: Get estimates $\hat{\beta}_0$ and $\hat{\beta}_1$, and therefore $\hat{\pi}$!


<!-- $$\small \pi_i = \frac{1}{1+\exp(-(\beta_0+\beta_1 x_{i}))} = \frac{1}{1 + \exp(-\eta_i)}$$ -->


$$\small \hat{\pi}(x) = \frac{1}{1+\exp(-\hat{\beta}_0-\hat{\beta}_1 x_{})}$$



::: notes

- with sample size $n$ and with $k$ predictors, we have the logistic regression model like this
- First, we have a probability distribution **Bernoulli** describing how the outcome or response data are generated. 
  - $Y_i \mid {\bf x}_i; \pi_i \sim \text{Bern}(p_i)$, ${\bf x}_i = (x_{1,i}, \cdots, x_{k,i})$, $i = 1, \dots, n$
- Then we have a link function, logit function, that relates the linear regression to the parameter of the outcome distribution, which is the parameter $p$, the probability of success in the Bernoulli distribution.
  - $\text{logit}(\pi_i) = \eta_i = \beta_0+\beta_1 x_{1,i} + \cdots + \beta_k x_{k,i}$

:::



## Probability Curve

:::: {.columns}

::: {.column width="40%"}
- The relationship between $\pi(x)$ and $x$ is not linear!
$$\pi(x) = \frac{1}{1+\exp(-\beta_0-\beta_1 x)}$$
- The amount that $\pi(x)$ changes due to a one-unit change in $x$ depends on the current value of $x$.
- Regardless of the value of $x$, if $\beta_1 > 0$, increasing $x$ will be increasing $\pi(x)$.
:::



::: {.column width="60%"}

```{r, ref.label="glm_default"}
#| echo: false
#| out-width: 100%
```

:::

::::


::: notes

```{r echo=FALSE, out.width="100%", cache=TRUE}
d <- tibble(eta = seq(-5, 5, length.out = 2000)) %>%
    mutate(logistic = (1/(1+exp(-eta))))

ggplot(d, aes(x = eta, y = logistic)) + 
    geom_line() + 
    xlim(-5,5) + 
    xlab("x") + 
    ylab(expression(pi)) +
    labs(title = expression(paste(pi, " vs. x"))) +
    theme_bw()
```

:::




## Interpretation of Coefficients {visibility="hidden"}

The ratio $\frac{\pi}{1-\pi} \in (0, \infty)$ is called the **odds** of some event.


- Example: If 1 in 5 people will default, the odds is 1/4 since $\pi = 0.2$ implies an odds of $0.2/(1−0.2) = 1/4$.



$$\ln \left( \frac{\pi(x)}{1 - \pi(x)} \right)= \beta_0 + \beta_1x$$

- Increasing $x$ by one unit 
  + changes the **log-odds** by $\beta_1$
  + multiplies the odds by $e^{\beta_1}$



:::{.alert}
- $\beta_1$ does *not* correspond to the change in $\pi(x)$ associated with a one-unit increase in $x$.
- $\beta_1$ is the change in **log odds** associated with one-unit increase in $x$.
:::


## Fit Logistic Regression

:::: {.columns}

::: {.column width="60%"}


```{r}
bodydata <- read_csv("./data/body.csv")
body <- bodydata |> 
    select(GENDER, HEIGHT) |> 
    mutate(GENDER = as.factor(GENDER))
body |> slice(1:4)
```


- `GENDER = 1` if Male

- `GENDER = 0` if Female

- Use `HEIGHT` (centimeter, 1 cm = 0.39 in) to predict/classify `GENDER`: whether one is male or female.
:::

::: {.column width="40%"}

:::{.tiny}

```{r}
#| echo: false
#| out-width: 100%
#| fig-cap: "Source: https://www.thetealmango.com/featured/average-male-and-female-height-worldwide/"
knitr::include_graphics("./images/17-logistic-reg/height.jpeg")
```

:::

:::

::::


## Logistic Regression - Data Summary

```{r}
table(body$GENDER)
```

```{r}
#| out-width: 100%
body |> ggplot(aes(x = GENDER, y = HEIGHT)) + geom_boxplot()
```


<!-- :::: {.columns} -->

<!-- ::: {.column width="60%"} -->
<!-- ```{r} -->
<!-- table(body$GENDER) -->
<!-- tapply(body$HEIGHT, body$GENDER, summary) -->
<!-- ``` -->
<!-- ::: -->



<!-- ::: {.column width="40%"} -->
<!-- ```{r} -->
<!-- #| out-width: "100%" -->
<!-- body |> ggplot(aes(x = GENDER,  -->
<!--                    y = HEIGHT)) + -->
<!--     geom_boxplot() -->
<!-- ``` -->
<!-- ::: -->
<!-- :::: -->


## Logistic Regression - Model Fitting

- Specify the model with `logistic_reg()` [![parsnip](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){width=60}](https://parsnip.tidymodels.org/reference/logistic_reg.html#:~:text=logistic_reg()%20defines%20a%20generalized,by%20setting%20the%20model%20engine.)

- Use `"glm"` instead of `"lm"` as the engine

```{r}
library(tidymodels)
show_engines("logistic_reg")
```


<!-- # ```{r} -->
<!-- # logis_mdl <- parsnip::logistic_reg() |>  -->
<!-- #     set_engine("glm")  -->
<!-- # ``` -->

<!-- ## Logistic Regression - Model Fitting -->



## Logistic Regression Model Result {visibility="hidden"}

```{r}
#| echo: true
# logis_out$fit$coefficients
```


- $\hat{\eta} = \ln \left( \frac{\hat{\pi}}{1 - \hat{\pi}}\right) = -40.55 + 0.24 \times \text{HEIGHT}$

<!-- . . . -->

<!-- - $\hat{\eta}(x) = \hat{\beta}_0 + \hat{\beta}_1x$ -->
<!-- - $\hat{\eta}(x+1) = \hat{\beta}_0 + \hat{\beta}_1(x+1)$ -->
<!-- - $\hat{\eta}(x+1) - \hat{\eta}(x) = \hat{\beta}_1 = \ln(\text{odds}_{x+1}) - \ln(\text{odds}_{x}) = \ln \left( \frac{\text{odds}_{x+1}}{\text{odds}_{x}} \right)$ -->

. . .

- One cm increase in `HEIGHT` increases the *log odds* of being male by 0.24 units.
- The **odds ratio**, $\widehat{OR} = \frac{\text{odds}_{x+1}}{\text{odds}_{x}} = e^{\hat{\beta}_1} = e^{0.24} = 1.273$.
- The odds of being male increases by 27.3% with additional one cm of `HEIGHT`.



## Logistic Regression - Model Fitting

- Define `family = "binomial"`

```{r}
logis_out <- logistic_reg() |> 
    fit(GENDER ~ HEIGHT, 
        data = body, 
        family = "binomial")
logis_out_fit <- logis_out$fit
```

. . .

```{r}
logis_out_fit$coefficients
```

. . .

#### Pr(GENDER = 1) When HEIGHT is 170 cm


<!-- - $\hat{\eta} = \ln \left( \frac{\hat{\pi}}{1 - \hat{\pi}}\right) = -40.55 + 0.24 \times \text{HEIGHT}$ -->


$$ \hat{\pi}(x = 170) = \frac{1}{1+\exp(-\hat{\beta}_0-\hat{\beta}_1 x)} = \frac{1}{1+\exp(-(-40.55) - 0.24 \times 170)} = 63.3\%$$


```{r}
predict(logis_out_fit, newdata = data.frame(HEIGHT = 170), type = "response")
```

<!-- . . . -->

<!-- :::{.question} -->
<!-- What is the probability of being male when `HEIGHT` is 160? What about `HEIGHT` 180? -->
<!-- ::: -->






## Probability Curve

```{r}
pi_hat <- predict(logis_out$fit, type = "response")
pi_hat |> head()
body$HEIGHT |> head()
```


<!-- ```{r} -->
<!-- predict(logis_out$fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = "response") -->
<!-- ``` -->

:::: {.columns}

::: {.column width="65%"}

```{r}
#| echo: false
#| label: default-predict-viz
#| out-width: 85%

height_0 <- body$HEIGHT[body$GENDER == 0]
height_1 <- body$HEIGHT[body$GENDER == 1]
newdata <- data.frame(HEIGHT = sort(body$HEIGHT))
pi_hat <- predict(logis_out$fit, newdata = newdata, type = "response")
par(mar = c(4, 4, 0, 0), mgp = c(2, 0.5, 0), las = 1)
plot(sort(body$HEIGHT), pi_hat, col = 4, xlab = "HEIGHT (cm)",
     ylab = "Probability of GENDER = 1", type = "l", lwd = 6,
     cex.axis = 1.2, cex.lab = 1.8)
points(height_0, rep(0, length(height_0)), pch = 3, cex = 0.5,
       col = alpha("black", alpha = 0.5))
points(height_1, rep(1, length(height_1)), pch = 3, cex = 0.5,
       col = alpha("red", alpha = 0.5))
abline(h = 0.5, lwd = 0.5, lty = 2)

pi_new <- predict(logis_out$fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), 
                  type = "response")
points(c(160, 170, 180), pi_new, pch = c(15, 16, 17), cex = 4,
       col = c("#ffb3a3", "#d1bc26", "#18ad90"))
```

:::

::: {.column width="30%"}

- **[`r paste0(160, " cm, Pr(male) = ", round(pi_new[1], 2))`]{.pink}**
- **[`r paste0(170, " cm, Pr(male) = ", round(pi_new[2], 2))`]{.yellow}**
- **[`r paste0(180, " cm, Pr(male) = ", round(pi_new[3], 2))`]{.green}**

:::

::::



##
::: {.lab}

<span style="color:blue"> **21-Logistic Regression** </span>

In **lab.qmd** `## Lab 21` section,

- Use our fitted logistic regression model to predict whether you are male or female! Change `175` to your height (cm).

- Use [the converter](https://www.rapidtables.com/convert/length/feet-inch-to-cm.html) to get your height in cm!

```{r}
#| eval: true

# Fit the logistic regression

predict(logis_out_fit, newdata = data.frame(HEIGHT = 175), 
        type = "response")
```

<!-- - Suppose data are collected with $X =$ hours studied, and $Y =$ receive an A or not in 3570. We fit a logistic regression and the estimated coefficients are $b_0 = −3$, $b_1 = 0.05$. -->
<!--     + Estimate the probability that a student who studies for 40 hours gets an A in the class. -->
<!--     + How many hours would the student need to study to have a 50% chance of getting an A in the class? -->
:::



# {background-color="#ffde57" background-image="https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg" background-size="40%" background-position="90% 50%"}


::: {.left}
<h1> sklearn.linear_model </h1>

:::


## [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)

```{r}
#| eval: false
library(reticulate); py_install("scikit-learn")
```


```{python}
#| eval: true
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
```

. . .

<br>


```{python}
#| eval: true
body = pd.read_csv('./data/body.csv')
x = np.array(body[['HEIGHT']]) ## 2d array with one column
y = np.array(body['GENDER']) ## 1d array
```

```{python}
x[0:4]
y[0:4]
```


## [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)


```{python}
#| eval: true
clf = LogisticRegression().fit(x, y)
clf.coef_
clf.intercept_
```

. . .

<br>

```{python}
new_height = np.array([160, 170, 180]).reshape(-1, 1)
```


```{python}
clf.predict(new_height)
```

. . .

<br>

```{python}
clf.predict_proba(new_height)
```


# Evaluation






<!-- # Evaluation Metrics -->

## Evaluation Metrics^[More on [Wiki page](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).]

- **Confusion Matrix**

|                        | True 0               | True 1            |
|------------------------|-------------------------------|-------------------------------|
| **Predict 0** |  **True Negative  (TN)** | **False Negative (FN)**|
| **Predict 1**  |  **False Positive (FP)**|  **True Positive  (TP)**           | 



- **Sensitivity (True Positive Rate)** $= P( \text{predict 1} \mid \text{true 1}) = \frac{TP}{TP+FN}$

- **Specificity (True Negative Rate)** $= P( \text{predict 0} \mid \text{true 0}) = \frac{TN}{FP+TN}$ 

- **Accuracy** $= \frac{TP + TN}{TP+FN+FP+TN}$

<!-- . . . -->

<!-- A *good* classifier is one which the *test accuracy rate is highest*. -->



## Confusion Matrix {visibility="hidden"}

```{r}
#| label: confusion

pred_prob <- predict(logis_out_fit, type = "response")

## true observations
gender_true <- body$GENDER

## predicted labels
gender_pred <- (pred_prob > 0.5) * 1

## confusion matrix
table(gender_pred, gender_true)
```


## Receiver Operating Characteristic (ROC) Curve
|                        | True 0               | True 1            |
|------------------------|-------------------------------|-------------------------------|
| **Predict 0** |  **True Negative  (TN)** | **False Negative (FN)**|
| **Predict 1**  |  **False Positive (FP)**|  **True Positive  (TP)**           |

- **Receiver operating characteristic (ROC) curve** plots True Positive Rate (Sensitivity) vs. False Positive Rate (1 - Specificity)


```{r, echo=FALSE, out.width="50%"}
library(ROCR)
# create an object of class prediction 
pred <- ROCR::prediction(
    predictions = pred_prob, 
    labels = gender_true)

# calculates the ROC curve
roc <- ROCR::performance(
    prediction.obj = pred, 
    measure = "tpr",
    x.measure = "fpr")
par(mar = c(5, 5, 0, 3))
par(cex.axis = 1.5, las = 1)
plot(roc, colorize = TRUE, cex.lab = 2, lwd = 4, cex.axis = 1.5)
```



## [ROC Curve](https://yardstick.tidymodels.org/reference/roc_curve.html) using [![yardstick of Tidymodels](https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png){width=60}](https://yardstick.tidymodels.org/)  {visibility="hidden"}


<!-- ![yardstick of Tidymodels](https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png){width="50"} -->
<!-- [yardstick of Tidymodels](https://yardstick.tidymodels.org/) -->



:::: {.columns}

::: {.column width="50%"}
```{r}
roc_df <- tibble(truth = gender_true,
                 prob = 1 - pred_prob,
                 pred = gender_pred)
roc_df
```
:::


::: {.column width="50%"}

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- knitr::include_graphics("https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png") -->
<!-- ``` -->
```{r}
gender_roc <- 
     yardstick::roc_curve(roc_df, truth, prob)
gender_roc |> slice(1:5, (n()-4):n()) |> 
    print(n = 10)
```
:::
::::

## ROC Curve using [![yardstick of Tidymodels](https://raw.githubusercontent.com/rstudio/hex-stickers/main/thumbs/yardstick.png){width=60}](https://yardstick.tidymodels.org/)  {visibility="hidden"}


```{r, echo=1}
#| out-width: "67%"
gender_roc |> autoplot() +
    theme(axis.title = element_text(size = 20),
          axis.text = element_text(size = 16))
```


::: notes
 + theme(axis.title = element_text(size = 20),
          axis.text = element_text(size = 16))
:::

## Comparing Models

:::{.question}
Which model performs better?
:::

```{r}
#| echo: false
#| out-width: "70%"
bodydata <- bodydata |> mutate(GENDER = as.factor(GENDER))
logis_out_all <- logistic_reg() %>%
    set_engine("glm") %>%
    fit(GENDER ~ ., data = bodydata, 
        family = "binomial")
prob_all <- predict(logis_out_all$fit, type = "response")
roc_df_all <- tibble(truth = gender_true, prob = 1 - prob_all)
roc_df_2 <- bind_rows(roc_df, roc_df_all)
roc_df_2 <- mutate(roc_df_2, model = c(rep("model-1", nrow(roc_df)), rep("model-2", nrow(roc_df))))
roc_df_2 |> group_by(model) |> roc_curve(truth, prob) |> autoplot() + 
    theme_bw() +
    theme(line = element_line(linewidth = 0),
          axis.title = element_text(size = 20),
          axis.text = element_text(size = 16),
          legend.title = element_text(size = 18),
          legend.text = element_text(size = 16))
```
