---
title: 'Linear Regression `r fontawesome::fa("chart-line")`'
subtitle: "MATH/COSC 3570 Introduction to Data Science"
author: "Dr. Cheng-Han Yu <br> Department of Mathematical and Statistical Sciences <br> Marquette University"
# date: "`r format(Sys.time(), '%B %d %Y')`"
# macros: _macros.tex # import a list of TeX/LaTeX definitions
format: 
  revealjs:
    #     - "macros.tex"
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    # include-in-header:
    highlight-style: arrow
    code-block-bg: true
    self-contained: false
    slide-number: c/t    
    incremental: false
    width: 1800
    height: 1000
    margin: 0.05
    logo: "https://upload.wikimedia.org/wikipedia/commons/e/e8/Marquette_Golden_Eagles_logo.svg"
    footer: "[math3570-s25.github.io/website](https://math3570-s25.github.io/website/)"
    theme: ["simple", "slides.scss"]
    multiplex: true
    code-link: true
    fig-cap-location: bottom
    fig-align: center
    transition: none ## fade slide convex concave zoom
    code-line-numbers: false
    title-slide-attributes:
      data-background-color: "#447099"
      # data-background-image: images/paper-texture.jpg
      # data-background-size: cover
      # data-background-color: "#698ED5"
editor: source
execute:
  freeze: true
  echo: true
---

#  {visibility="hidden"}

\def\bx{\mathbf{x}}
\def\bg{\mathbf{g}}
\def\bw{\mathbf{w}}
\def\bbeta{\boldsymbol \beta}
\def\bX{\mathbf{X}}
\def\by{\mathbf{y}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\T{\text{T}}
\def\cov{\mathrm{Cov}}
\def\cor{\mathrm{Corr}}
\def\var{\mathrm{Var}}
\def\E{\mathrm{E}}
\def\bmu{\boldsymbol \mu}
\DeclareMathOperator*{\argmin}{arg\,min}
\def\Trace{\text{Trace}}



```{r}
#| label: setup
#| include: false
#| eval: true
library(countdown)
library(emo)
library(knitr)
library(gt)
library(gtExtras)
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(fontawesome)
library(rmarkdown)
library(reticulate)
library(lattice)
library(openintro)
library(plotly)
# library(ISLR2)
# library(genridge)
# library(glmnet)
# library(gam)
# library(splines)
# library(MASS)

# library(ElemStatLearn)
knitr::opts_chunk$set(
    fig.asp = 0.618,
    fig.align = "center",
    out.width = "100%",
    fig.retina = 10,
    fig.path = "images/16-linear-reg/",
    message = FALSE,
    global.par = TRUE
)
options(
  htmltools.dir.version = FALSE,
  dplyr.print_min = 6, 
  dplyr.print_max = 9,
  tibble.width = 80,
  width = 80,
  digits = 3
  )
hook_output <- knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})

theme_set(theme_minimal(base_size = 20))
```


## {visibility="hidden"}

<br>

<br>

<br>

<br>

<br>

:::{.center}

### Simple Linear Regression (MATH 4720)
### Categorical Predictors (MATH 2780/4780)
### Multiple Linear Regression (MATH 2780/4780)

:::



## What is Regression

- **Regression** models the relationship between a **numerical response variable $(Y)$** and one or more **numerical/categorical predictors $(X)$**, which is a **supervised learning** method in **machine learning**.

- A **regression function** $f(X)$ describes how a response variable $Y$ generally changes as an explanatory variable $X$ changes.


:::: {.columns}

::: {.column width="50%"}

Examples:

- <span style="color:blue"> college GPA $(Y)$ vs. ACT/SAT score $(X)$</span>

- <span style="color:blue"> sales $(Y)$ vs. advertising expenditure $(X)$</span>

- <span style="color:blue"> crime rate $(Y)$ vs. median income level $(X)$ </span>

:::


::: {.column width="50%"}

```{r}
#| echo: false
#| purl: false
#| out-width: 100%
par(mar = c(2, 2, 0, 0), mgp = c(1, 0.2, 0))
x <- runif(100, 0, 10)
y_linear <- 5 + 2 * x + rnorm(100, sd = 1)
y_quad <- log(x) + rnorm(100, sd = 0.3)
plot(x, y_quad, pch = 16, col = 4, xlab = "X", ylab = "Y", xaxt='n', yaxt = "n")
lines(sort(x), log(sort(x)), col = 2, lwd = 3)
legend("bottomright", c("data points", "f(X)"), col = c(4, 2), lwd = c(NA, 3), pch = c(16, NA), bty = "n")
```

:::

::::


## Simple Linear Regression

<!-- Given the **training data** $(x_1, y_1), \dots, (x_n, y_n)$, we *learn* (estimate) -->

:::: {.columns}

::: {.column width="40%"}

\begin{align*}
y_i &= f(x_i) +  \epsilon_i \\
    &= \beta_0 + \beta_1~x_{i} + \epsilon_i, \quad i = 1, 2, \dots, n
\end{align*}

- $\beta_0$ and $\beta_1$ are *unknown parameters* to be learned or estimated.

:::

::: {.column width="60%"}

```{r}
#| purl: false
#| echo: false
#| out-width: 100%
load("../../../intro_stats/OTT_Final/R_WORKSPACE/CH11/table11-1.rdata")
data <- `table11-1`
data <- data[order(data$x), ]
par(mar = c(3, 3, 0, 0))
plot(data$x, data$y, xlab = "x", ylab = "y", pch = 19, col = "#003366", las = 1,
     main = "", type = "p")
lm_out <- lm(data$y~data$x)
abline(lm_out, col = 4, lwd = 2)
abline(a = coef(lm_out)[1]*0.8, b = coef(lm_out)[2]*1.05, col = 2, lwd = 2)
legend("bottomright", c("true unknown population regression line", "sample fitted regression line"), col = c(2, 4), lwd = 3, bty = "n")
```

:::

::::

. . .

:::{.question}
What are the assumption on $\epsilon_i$?
:::

. . .

$\epsilon_i \sim N(0, \sigma^2)$ and hence $y_i \mid x_i \sim N(\beta_0+\beta_1x_i, \sigma^2)$ 

<!-- or $\mu_{y\mid x_i} = \beta_0+\beta_1x_i$. -->



::: notes
- OK we start with the simple linear regression, the linear regression a Single Predictor $x$.
- In linear regression, our regression function $f$ is a linear function $\beta_0 + \beta_1~x$. 
- Epsilon, the random error is there to capture any random measurement errors or any variations in $y$ that cannot be explained by the predictor $x$.
- Given this model, we're interested in $\beta_0$ (population parameter for the intercept) and $\beta_1$ (population parameter for the slope) because once we know $\beta_0$ and $\beta_1$, we know the exact shape of $f$ and we know the relationship of $y$ and $x$, and given any value of $x$, we can predict its corresponding value of $y$ using the regression line $\hat{y}_{i} = \beta_0 + \beta_1~x_{i}$.
- But unfortunately, the population parameters are typically unknown to us.
:::



## Simple Linear Regression Assumptions

```{r}
#| purl: false
#| echo: false
#| out-width: 100%
knitr::include_graphics("./images/16-linear-reg/regression_line_sig_red.png")
```



## Simple Linear Regression Assumptions

```{r}
#| purl: false
#| echo: false
#| out-width: 100%
knitr::include_graphics("./images/16-linear-reg/regression_line_data.png")
```




## Simple Linear Regression Assumptions

```{r}
#| purl: false
#| echo: false
#| out-width: 100%
knitr::include_graphics("./images/16-linear-reg/regression_line_data_blue.png")
```



## Ordinary Least Squares (OLS)

Given the **training data** $(x_1, y_1), \dots, (x_n, y_n)$, use *sample statistics* $b_0$ and $b_1$ computed from the data to

- [inference:]{.green} estimate $\beta_0$ and $\beta_1$ 

- [fitting:]{.green} estimate $y_i$ or $f(x_i)$ at $x_i$ by its **fitted value** $$\hat{y}_{i} = \hat{f}(x_i) = b_0 + b_1~x_{i}$$

- [prediction:]{.green} predict $y_j$ or $f(x_j)$ at $x_j$ by its **predicted value** $$\hat{y}_{j} = \hat{f}(x_j) = b_0 + b_1~x_{j}$$ where $(x_j, y_j)$ is *never seen and used in training* before.

. . .

- [Ordinary Least Squares:]{.green} Find $b_0$ and $b_1$, or regression line $b_0 + b_1x$ that minimizes the **sum of squared residuals**.
- The residual $e_i = y_i - \hat{y}_i$. The sample regression line minimizes $\sum_{i = 1}^n e_i^2$.



::: notes
- How do we get  $b_0$ and $b_1$ that sort of well estimate $\beta_0$ and $\beta_1$? 
- We choose $b_0$ and $b_1$, or regression line $b_0 + b_1x$ that minimizes the **sum of squared residuals**.
- If we define residual as $e_i = y_i - \hat{y}_i$, then the sum of squared residuals is $\sum_{i = 1}^n e_i^2$.
- And this approach that estimates the population parameters $\beta_0$ and $\beta_1$ or the population regression line is called Ordinary Least Squares method.
:::


## Visualizing Residuals

```{r}
#| purl: false
#| echo: false
#| label: vis-res-1
#| out-width: 80%
mpg_data <- mpg
# mpg_data$displ <- mpg_data$displ + rnorm(length(mpg$displ), 0, 1)
# mpg_data$hwy <- mpg_data$hwy
reg_fit <- linear_reg() |> 
    set_engine("lm") |> 
    fit(hwy ~ displ, data = mpg_data)

reg_fit_tidy <- tidy(reg_fit$fit) 
reg_fit_aug  <- augment(reg_fit$fit) %>%
    mutate(res_cat = ifelse(.resid > 0, TRUE, FALSE))

p <- ggplot(data = reg_fit_aug, 
            aes(x = displ, y = hwy)) +
     geom_point(alpha = 0.3) + 
     labs(title = "Highway MPG vs. Engine Displacement",
          x = "Displacement (litres)",
          y = "Highway miles per gallon") +
      coord_cartesian(ylim = c(11, 44))
p + theme(plot.subtitle = element_text(colour = "red", 
                                       face = "bold", 
                                       size = rel(1.5))) +
    labs(subtitle = "Just the data")
```

::: notes
OK. That's see the idea of Ordinary Least Squares visually. Here just showed the data.
- Do you see why some points are darker than some others?
- A darker point means that there are several identical (x, y) pairs, or replicates in the data set.
:::


## Visualizing Residuals (cont.)

```{r}
#| purl: false
#| echo: false
#| label: vis-res-2
#| out-width: 80%
#| cache: false
p + geom_smooth(method = "lm", color = "#003366", se = FALSE) +
    geom_point(mapping = aes(y = .fitted), color = "red", size = 1) +
    theme(plot.subtitle = element_text(colour = "red", 
                                       face = "bold", 
                                       size = rel(1.5))) +
    labs(subtitle = "Data + least squares line + fitted value")
```

::: notes
- All right, with the data, this figure also shows the least squares regression line, and the fitted (predicted) value of $y$ for each $x$ in the training data, which are those red points.
- The predicted values of y are right on the regression line.
- Now the question is, how do we find this line?
- Given a line, we can have predicted values of y, right?
- Then what is residual on the plot? The residual will be the difference between the true observation y and the predicted value of y given any value of x.
- So a residual in the plot will be a vertical bar at the value of x with two ends of the bar $y$ and $\hat{y}$, right?
- (Show on board)
- (add $y_i = b_0+b_1x_i$ and residual line)
:::



## Visualizing Residuals (cont.)

```{r}
#| purl: false
#| echo: false
#| label: vis-res-3
#| out-width: 80%
#| cache: false
p + geom_segment(mapping = aes(xend = displ, yend = .fitted),
                 color = "red", alpha = 0.4) +
  geom_smooth(method = "lm", color = "#003366", se = FALSE) +
  theme(plot.subtitle = element_text(colour = "red", 
                                       face = "bold", 
                                       size = rel(1.5))) +
  labs(subtitle = "Data + least squares line + residuals")
```


::: notes
- Here shows all the residuals in vertical bars.
- least squares line is the line such that the sum of all the squared residuals is minimized.
- Why we square the residuals? 
- It's mathematically more convenient.
- Squaring emphasizes larger differences
:::


# Fitting and Interpreting Regression Models


## Predict Highway MPG `hwy` from Displacement `displ`

$$\widehat{hwy}_{i} = b_0 + b_1 \times displ_{i}$$

```{r}
#| purl: false
#| echo: false
#| label: hwy-displ-plot
#| out-width: 90%
#| cache: true
ggplot(data = mpg, aes(x = displ, y = hwy)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "#003366") + 
    labs(
      title = "Highway MPG vs. Engine Displacement (litres)",
      x = "Displacement (litres)",
      y = "Highway MPG"
    )
```

::: notes
- The data I just show you is the mpg data set in ggplot2.
- Here we are trying to Predict Highway MPG `hwy` from Engine Displacement `displ`
:::



## R Built-in `lm()`  {visibility="hidden"}

```{r}
## lm_reg <- lm(formula = mpg$hwy ~ mpg$displ)
(lm_reg <- lm(formula = hwy ~ displ, data = mpg))
```

$$\widehat{hwy}_{i} = 35.7  -3.53 \times displ_{i}$$

<!-- ## Slope and Intercept -->

<!-- $$\widehat{hwy}_{i} = 35.7  -3.53 \times displ_{i}$$ -->


- **Slope:** When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, **on average**, by 3.53 miles.
- **Intercept:** Cars that has engine displacement 0 litres are expected to have highway miles per gallon 35.7, on average. *(Does this make sense?)*


::: notes
- All right, so how do we interpret the regression line.
- The interpretation of the slope is that When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, on average, by 3.351 miles.
- Notice that I said on average. When displacement volume increases one unit, the highway MPG will not decrease 3.351 miles for every single car. Each car will have a different MPG, but on average, MPG decreases by 3.351 miles
- And the intercept means Cars that has engine displacement 0 litres are expected to have highway miles per gallon 35.698, on average. *(Does this make sense?)*
- Mathematically speaking, it's valid to say something like this. But in reality, there is no cars having zero displacement, it does not make sense to give this kind of statement. 
- When you say something about your model beyond the scope or range of your data, you got to be very careful. The statement may not be realistic, and the model assumption like linear relationship may not be reasonable or violated.
:::




## R Built-in `lm()`  {visibility="hidden"}

```{r}
#| label: lm
typeof(lm_reg)
names(lm_reg)
```


```{r}
lm_reg$fitted.values[1:5]
mpg$hwy[1:5]
mpg$displ[1:5]
```


## R Built-in `lm()`  {visibility="hidden"}
```{r}
#| class-output: my_class800
summary(lm_reg)
```




# {background-color="#A7D5E8"}

![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tidymodels.png){width="380"}


![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/rsample.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/recipes.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/workflows.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/tune.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/yardstick.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/broom.png){width="180"}![](https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/dials.png){width="180"}


<h1>[Tidymodels](https://www.tidymodels.org/)</h1>






## Step 1: Specify Model:

[`linear_reg()`](https://parsnip.tidymodels.org/reference/linear_reg.html)

```{r}
library(tidymodels)
parsnip::linear_reg()
```

[**parsnip**](https://parsnip.tidymodels.org/) package provides a tidy, unified interface for fitting models

```{r}
#| echo: false
#| out-width: 30%
knitr::include_graphics("https://raw.githubusercontent.com/rstudio/hex-stickers/master/PNG/parsnip.png")
```


::: notes
One challenge with different modeling functions available in R that do the same thing is that they can have different interfaces and arguments. Note that the model syntax can be very different and that the argument names (and formats) are also different. This is a pain if you switch between implementations. 
- So how do we use tidymodels to actually fit a model. 
- Step 1 is to specify the model that we are building.
- We are going to build a linear regression model, so the function we start with is linear_reg()
- And you can see that the output is saying OK I'm ready to use this model specification.
:::



## Step 2: Set Model Fitting *Engine*

- By default, use `lm()` in the built-in **stats** package.

```{r}
linear_reg() |> 
    set_engine("lm")
```

. . .

```{r}
show_engines("linear_reg")
```


::: notes
- Step 2 is to define the model fitting engine.
- For now we are going to use "lm" that is a function in the stats package to fit a linear model. 
- Basically, we ask R or tidymodel to use the lm() function as our computational engine to fit a linear regression model.
- So you can think tidymodel as an platform or interface with many other R packages that do the work of model fitting, and tidymodel provides a consistent interface to them.
- So you can actually use different R packages as different computational engines to fit the same model.
- With tidymodel, you don't need to worry about different syntax should be used in different packages because the syntax is the same, and its outputs also remain the same even different computational engine is used.
:::




## Step 3: Fit Model & Estimate Parameters

... using **formula syntax**


```{r fit-model}
linear_reg() |> 
    fit(hwy ~ displ, data = mpg)
```


$$\widehat{hwy}_{i} = 35.7  -3.53 \times displ_{i}$$


- **Slope:** When the engine displacement volume of a car is increased by one litre, the highway miles per gallon is expected to be lower, *on average*, by 3.53 miles.

<!-- - **Intercept:** Cars that has engine displacement 0 litres are expected to have highway miles per gallon 35.7, on average. *(Does this make sense?)* -->



::: notes
- Step 3, after selecting the computational engine, we fit the model and estimate parameters.
- we use the fit() function, and inside the function, we provide the model fitting formula, response variable ~ predictors, followed by the data set's name.
- Here our response is hwy, and the predictor is displ, and the data set is mpg.
- After fitting the model, we can the model output. We can see our estimated coefficients intercept and slope for displ, which are $b_0$ and $b_1$ in previous slides.
- And with the coefficients, we now can have the regression line, $\widehat{hwy}_{i} = 35.698  -3.531 \times displ_{i}$, where $\widehat{hwy}_{i}$ is the predicted value of hwy MPG of the $i$-th observation.
:::



## Tidy Look at Model Output

- Tidymodels output (**tibble**)

```{r}
linear_reg() |> 
    fit(hwy ~ displ, data = mpg) |> 
    tidy()
```

$$\widehat{hwy}_{i} = 35.7  -3.53 \times displ_{i}$$

::: notes
- We can print the model output as a tidy table using the tidy() function. 
- Here we not only have the point estimates in the fist column, but also have the standard error that measure the uncertainty of these coefficients, the test statistics, and p-values that are used in hypothesis testing.
- We'll use standard error when quantifying the uncertainty about the regression line, but we won't talk about test statistics and p-values in this course because they are covered in 4720.
:::






##
:::{.lab}

<span style="color:blue"> **20-Simple Linear Regression** </span>

In **lab.qmd** `## Lab 20` section,

- Use the `mpg` data to fit a simple linear regression where $y$ is `hwy` and $x$ is `cty`.

- Produce the plot below. (add the layer `geom_smooth(method = "lm", se = FALSE)`)

```{r}
#| out-width: 50%
#| echo: !expr c(1)
library(tidymodels); library(ggplot2)
ggplot(data = mpg, aes(x = cty, y = hwy)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "#003366") + 
    labs(title = "Highway MPG vs. City MPG",
         x = "City MPG",
         y = "Highway MPG") +
    theme_bw()
```
:::




::: notes
```{r}
#| eval: false
parsnip::linear_reg() |> 
    fit(hwy ~ cty, data = mpg) |>  
    tidy()
```

```{r}
#| eval: false
library(tidymodels); library(ggplot2)
ggplot(data = mpg, aes(x = cty, y = hwy)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "#003366") + 
    labs(title = "Highway MPG vs. City MPG",
         x = "City MPG",
         y = "Highway MPG")
```
:::




## Quantify Uncertainty about Coefficients

- Uncertainty about regression coefficients $\beta_0$ and $\beta_1$

::: {.fragment .fade-in-then-semi-out}

```{r}
reg_out <- linear_reg() |> 
    fit(hwy ~ displ, data = mpg)
names(reg_out)
```

:::

::: {.fragment .fade-in-then-semi-out}

```{r}
reg_out_fit <- reg_out$fit
names(reg_out_fit)
reg_out_fit$coefficients
```

:::

::: {.fragment .fade-up}

```{r}
confint(reg_out_fit)
```

:::



## Quantify Uncertainty about Mean of $y$

<!-- :::: {.columns} -->

<!-- ::: {.column width="40%"} -->

- Uncertainty about the [**mean**]{.green} value of $y$ given $X = x$
$$\mu_{Y \mid X = x} = \beta_0 + \beta_1x$$

```{r}
new_input <- data.frame(displ = 3:6)

predict(reg_out_fit, newdata = new_input, interval = "confidence", level = 0.95)
```

<!-- ::: -->



<!-- ::: {.column width="60%"} -->



<!-- ::: -->
<!-- :::: -->


::: notes
- We are also interested in uncertainty about the mean value of $y$ given some value of $x$, especially when we are predicting $y$.
- To get the CI for the mean of $y$ at any value of $x$, we can use the predict() function.
- The first argument in the function is the regression fitted result.
- Then and argument new data is a data frame of predictor values.
- Here, the predictor is at 1, 2, 3, up to 8.
- And interval is confidence, and confidence level is 95%.
- The output will have 3 columns. The first column shows the predicted or fitted value of $y$ given $x$, these are values right on the regression line.
- The second and third columns are lower and upper bound respectively.
- How do we show the confidence interval for the mean of y?
- Again we use geom_smooth() layer, and set se = TRUE, so that geom_smooth() uses the standard error info to obtain the confidence interval for us. 
:::




## Quantify Uncertainty about Mean of $y$ 

::: {.panel-tabset}

## Fit with uncertainty band

```{r}
#| out-width: 70%
#| echo: false
(p_ci <- p + geom_smooth(method = "lm", 
                         color = "#003366", 
                         fill = "blue",
                         se = TRUE))
```

## ggplot code

```{r}
#| eval: false
p <- ggplot(data = reg_out_fit, 
            aes(x = displ, y = hwy)) +
     geom_point(alpha = 0.3) + 
     labs(title = "Highway MPG vs. Engine Displacement",
          x = "Displacement (litres)",
          y = "Highway miles per gallon") +
      coord_cartesian(ylim = c(11, 44))

p_ci <- p + geom_smooth(method = "lm", 
                        color = "#003366", 
                        fill = "blue",
                        se = TRUE)
```


:::





## Quantify Uncertainty about Individual $y$

<!-- :::: {.columns} -->

<!-- ::: {.column width="40%"} -->

- Uncertainty about the [**individual**]{.green} value of $y$ given $X = x$, $Y \mid X = x$

```{r}
#| warning: false
predict(reg_out_fit, newdata = new_input, interval = "prediction", level = 0.95)
## predict at current inputs
pred_y <- predict(reg_out_fit, interval = "prediction")
head(pred_y)
```
<!-- ::: -->

<!-- ::: {.column width="60%"} -->
<!-- ```{r} -->
<!-- #| out-width: "100%" -->
<!-- p_ci +  -->
<!--     geom_line(aes(x = displ, y = df$lwr),  -->
<!--               color = "red") + -->
<!--     geom_line(aes(x = displ, y = df$upr),  -->
<!--               color = "red") -->
<!-- ``` -->
<!-- ::: -->
<!-- :::: -->


::: notes
- We can also quantify the uncertainty about the value individual $y$ given $x$.
- As you can imagine, this is more difficult to predict value of individual $y$ than to predict value of mean of $y$, right?
- Because individual $y$ value varies more than the mean of $y$ due to the random error noises added to it.
- And so with the same confidence level, the CI for individual $y$ will be much wider than the CI for the mean of $y$.
- How do we get the CI?
- The code is basically the same as before. But here instead of "confidence", here we use "prediction" in the interval argument.
- You can see that the 95% interval is much wider, trying to capture or contain most of the observed values of $y$.
:::


## Quantify Uncertainty about Individual $y$

```{r}
#| out-width: 100%
p_ci + 
    geom_line(aes(x = displ, y = pred_y[, "lwr"]), color = "red") +
    geom_line(aes(x = displ, y = pred_y[, "upr"]), color = "red")
```


# Model Checking

::: notes
- OK. Model checking. How do we know linear regression model is a good model for fitting our data. Well, we can assess the quality of the model by looking at residuals. Let's see how.
:::



<!-- ## Model Checking -->
<!-- - We assumes a **linear** relationship between our predictors and responses. -->
<!-- - But how do we assess the assumption? -->




## Graphical Diagnostics: Residual Plot

- Residuals distributed randomly around 0.
- Check it by plotting residuals against the fitted value of $y$: $e_i$ vs. $\hat{y}_i$
- With no visible pattern along the x or y axis.


```{r}
#| purl: false
#| echo: false
#| out-width: 56%
df <- tibble(
    fake_resid = rnorm(1000, mean = 0, sd = 30),
    fake_predicted = runif(1000, min = 0, max = 200)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
    geom_point(alpha = 0.5) +
    geom_hline(yintercept = 0, color = "red") +
    labs(x = "Predicted (y_hat)", y = "Residuals (e)")
```

::: notes
We are looking for
- Residuals distributed randomly around 0.
- With no visible pattern along the $x$ or $y$ axes.
- This can be checked by plotting residuals against the fitted value of $y$.
- Here shows a good residual plot. Residuals are around 0, and its variation is more or less the same across different values of $\hat{y}$.
- There is no significant pattern in the plot.
:::



## Not looking for...

:::{.large}
**Fan shapes**
:::

```{r}
#| purl: false
#| echo: false
#| out-width: 60%
set.seed(12346)
df <- tibble(
  fake_resid = c(rnorm(100, mean = 0, sd = 1), 
                 rnorm(100, mean = 0, sd = 15), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 20), 
                 rnorm(100, mean = 0, sd = 25), 
                 rnorm(100, mean = 0, sd = 50), 
                 rnorm(100, mean = 0, sd = 35), 
                 rnorm(100, mean = 0, sd = 40),
                 rnorm(200, mean = 0, sd = 80)),
  fake_predicted = seq(0.2, 200, 0.2)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(x = "Predicted (y_hat)", y = "Residuals (e)")
```

::: notes
- We are not looking for a plot that has a fan shape.
:::



## Not looking for...

:::{.large}
**Groups of patterns**
:::

```{r}
#| purl: false
#| echo: false
#| out-width: 60%
set.seed(12346)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = c(
    rnorm(500, mean = -20, sd = 10),
    rnorm(500, mean = 10, sd = 10)
  )
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(x = "Predicted (y_hat)", y = "Residuals (e)")
```


::: notes
- We are not looking for a plot that has **Groups of patterns**
:::



## Not looking for...

:::{.large}
**Residuals correlated with predicted values**
:::

```{r}
#| purl: false
#| echo: false
#| out-width: 60%
set.seed(12346)
df <- tibble(
  fake_predicted = seq(0.2, 200, 0.2),
  fake_resid = fake_predicted + rnorm(1000, mean = 0, sd = 50)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(x = "Predicted (y_hat)", y = "Residuals (e)")
```


::: notes
- We also don't want Residuals to be correlated with predicted values
:::



## Not looking for...

:::{.large}
**Any patterns!**
:::

```{r}
#| purl: false
#| echo: false
#| out-width: 60%
set.seed(12346)
df <- tibble(
  fake_predicted = seq(-100, 100, 0.4),
  fake_resid = -5*fake_predicted^2 - 3*fake_predicted + 20000 + rnorm(501, mean = 0, sd = 10000)
)
ggplot(df, mapping = aes(x = fake_predicted, y = fake_resid)) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0, color = "red") +
  labs(x = "Predicted (y_hat)", y = "Residuals (e)")
```

::: notes
- We basically don't want the residuals show Any patterns!
- If $X$ and $Y$ are not linearly related, or the residual plot exists some pattern, some data transformation is needed. Or use another model.
:::



## MPG Data Residuals

```{r}
#| out-width: 70%
#| echo: !expr c(-1)
par(mar = c(4, 4, 2, 0))
plot(reg_out_fit, which = 1, col = "blue", las = 1)
```






# Models with Categorical Predictors



## Categorical Predictor with 2 Categories

:::: {.columns}

::: {.column width="31%"}

```{r}
mpg |>  
  select(hwy, trans) |>  
  print(n = 4)
```

<!-- ```{r echo=FALSE, cache=TRUE, eval=FALSE} -->
<!-- mpg_data <- mpg -->
<!-- mpg_data$trans[grepl("auto", mpg_data$trans)] <- "auto" -->
<!-- mpg_data$trans[grepl("manual", mpg_data$trans)] <- "manual" -->

<!-- mpg_data |>  -->
<!--     select(hwy, trans) |>  -->
<!--     print(n = 8) -->
<!-- ``` -->

:::



::: {.column width="69%"}

::: {.fragment .fade-in-then-semi-out}

```{r}
#| purl: false
mpg_new <- mpg
mpg_new$trans[grepl("auto", mpg_new$trans)] <- "auto"
mpg_new$trans[grepl("manual", mpg_new$trans)] <- "manual"

mpg_new |> 
    select(hwy, trans) |> 
    print(n = 4)
```

:::

:::

::::

- `trans = auto`: Automatic transmission
- `trans = manual`: Manual transmission


::: notes
- If you look at the `mpg` data set, you'll find that the variable transmission is a categorical variable. We either have Automatic transmission or Manual transmission. 
- Here I clean the data a little bit, so that the trans variable has values either auto or manual.
- And I am going to use this trans variable as our predictor to predict the hwy MPG. OK.
:::


## Highway MPG & Transmission Type

- Make sure that your categorical variable is of type **character** or **factor**.

```{r}
typeof(mpg_new$trans)
```

```{r hwy-trans-fit}
linear_reg() |> 
    fit(hwy ~ trans, data = mpg_new) |> 
    tidy()
```

- The baseline level is chosen to be `auto` transmission. 


::: notes
- Before fitting the regression model, Make sure that your categorical variable is of type character or factor. If not, convert the type before fitting.
- After fitting the model, you can see that the name in the second row for the slope is transmanual, and it means something because R actually does something for us when fitting a regression model.
- Actually, it means that the baseline level is chosen to be auto transmission. 
:::


## Highway MPG & Transmission Type
```{r}
#| echo: false
linear_reg() |> 
    fit(hwy ~ trans, data = mpg_new) |> 
    tidy()
```

$$\widehat{hwy_{i}} = 22.3 + 3.49~trans_i$$

- **Slope:** Cars with **manual** transmission are expected, on average, to be 3.49 more miles per gallon than cars with **auto** transmission.
    - Compare baseline level (`trans = auto`) to the other level (`trans = manual`)
- **Intercept:** Cars with **auto** transmission are expected, on average, to have 22.3 highway miles per gallon.


::: notes
- Here the regression line is $\widehat{hwy_{i}} = 22.3 + 3.49~trans_i$.
- And we interpret the slope and intercept as follows.
- Cars with **manual** transmission are expected, on average, to be 3.48 more miles per gallon than cars with **auto** transmission.
- We Compare baseline level (`trans = auto`) to the other level (`trans = manual`).
- **Intercept** here is Cars with **auto** transmission are expected, on average, to have 22.3 highway miles per gallon.
- So basically, the intercept value 22.3 here is the average hwy MPG for cars with auto transmission, and intercept + slope (22.3 + 3.49) is the average hwy MPG for cars with manual transmission.
:::



# {background-color="#ffde57" background-image="https://upload.wikimedia.org/wikipedia/commons/0/05/Scikit_learn_logo_small.svg" background-size="40%" background-position="90% 50%"}


::: {.left}
<h1> sklearn.linear_model.LinearRegression </h1>

:::

## [sklearn.linear_model.LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)

```{r}
#| eval: false
library(reticulate)
py_install("scikit-learn")
```


```{python}
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
```

. . .

<br>

```{python}
mpg1 = pd.read_csv('./data/mpg.csv')
# x = np.array(mpg1['displ']).reshape(-1, 1)
x = np.array(mpg1[['displ']]) ## 2d array with one column
y = np.array(mpg1['hwy'])
x[0:4]
y[0:4]
```





::: notes
-1 in reshape function is used when you dont know or want to explicitly tell the dimension of that axis. E.g,
If you have an array of shape (2,4) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 8 rows, hence, (8,1).
The -1 signifies that NumPy should automatically calculate the number of rows based on the array's size, while 1 specifies that there should be only one column.
:::



## sklearn.linear_model.LinearRegression

```{python}
reg = LinearRegression().fit(x, y)
reg.coef_
reg.intercept_
```



. . .

<br>

```{python}
new_input = np.arange(3, 7, 1).reshape(-1, 1)
pred = reg.predict(new_input)
pred
```

<!-- ## Prediction -->


